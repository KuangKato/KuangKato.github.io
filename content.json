{"meta":{"title":"Better than yesterday","subtitle":null,"description":null,"author":"Kuanger","url":"http://yoursite.com"},"pages":[{"title":"categories","date":"2018-11-29T14:54:45.000Z","updated":"2018-11-29T14:56:33.380Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"About Me","date":"2018-11-29T14:55:25.000Z","updated":"2018-11-30T07:31:12.384Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"孔子云：取乎其上，得乎其中；取乎其中，得乎其下；取乎其下，则无所得矣。 “ 在科技道路上不断爬行的一个普通人。” 个人博客，用于分享一些在日常学习工作甚至于生活中遇到的一些比较有趣的东西。七荤八素，胡言乱语，望各位看官见谅。 个人联系方式 Email： kuang_kato@163.com Phone： 13168844985"},{"title":"tags","date":"2018-11-29T14:53:19.000Z","updated":"2018-11-29T14:55:58.165Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Jsoup的学习","slug":"spider/Jsoup的学习","date":"2018-12-10T03:57:00.000Z","updated":"2018-12-10T16:02:06.552Z","comments":true,"path":"2018/12/10/spider/Jsoup的学习/","link":"","permalink":"http://yoursite.com/2018/12/10/spider/Jsoup的学习/","excerpt":"摘要Jsoup是一款比较好的Java版HTML解析器。可直接解析某个URL地址、HTML文本内容。它提供了一套非常省力的API，可通过DOM，CSS以及类似于jQuery的操作方法来取出和操作数据。 由于工作原因，需要学习对页面的数据进行爬取，以下内容仅供学习demo，非商业用途。 学习途中查询了一些资料，也发现很多工具也可以实现，有兴趣可以前往了解 然后我发现jsoup可以对响应的html数据解析,于是对jsoup进行学习","text":"摘要Jsoup是一款比较好的Java版HTML解析器。可直接解析某个URL地址、HTML文本内容。它提供了一套非常省力的API，可通过DOM，CSS以及类似于jQuery的操作方法来取出和操作数据。 由于工作原因，需要学习对页面的数据进行爬取，以下内容仅供学习demo，非商业用途。 学习途中查询了一些资料，也发现很多工具也可以实现，有兴趣可以前往了解 然后我发现jsoup可以对响应的html数据解析,于是对jsoup进行学习 jSOUP主要功能 从一个URL，文件或字符串中解析HTML； 使用DOM或CSS选择器来查找、取出数据； 可操作HTML元素、属性、文本； 环境搭建 MAVEN依赖12345&lt;dependency&gt;&lt;groupId&gt;org.jsoup&lt;/groupId&gt;&lt;artifactId&gt;jsoup&lt;/artifactId&gt;&lt;version&gt;1.8.3&lt;/version&gt;&lt;/dependency&gt; ### 案例举例一 举例：获取携程的酒店名称12345678910@Testpublic void test1()&#123; Document document = Jsoup.connect(url) .timeout(10000) .referrer(\"http://hotels.ctrip.com\") .get(); System.out.println(document); Elements elements = document.select(\".cn_n\"); System.out.println(\"酒店名称：\"+elements.text());&#125; 输出 123456789...&lt;div class=\"htl_info\" id=\"J_htl_info\"&gt; &lt;div class=\"name\" itemtype=\"//schema.org/Hotel\"&gt; &lt;h2 class=\"cn_n\" itemprop=\"name\"&gt;河南大厦&lt;/h2&gt; &lt;h2 class=\"en_n\"&gt;Beijing Henan Plaza Hotel&lt;/h2&gt; &lt;span id=\"ctl00_MainContentPlaceHolder_commonHead_span_hotel_medal\" data-role=\"title\" class=\"medal \" title=\"\" rstar=\"2\" cpr=\"0\"&gt;&lt;/span&gt; &lt;span class=\"medal ico_quality_gold\" title=\"确认订单更快速，入住过程更顺利，携程服务品质认证。\" style=\"display:none\" id=\"J_ServiceScoreIcon\"&gt;品质保障&lt;/span&gt; &lt;/div&gt; ... 通过标签选择器定位就可以了,效果和element可以看上图 1酒店名称：河南大厦 举例二 举例：获取酒店详情中房型的价格 寻找url 由于酒店房型数据是Ajax的，需要在开发者工具中（F12）寻找url1234567原url：http://hotels.ctrip.com/Domestic/tool/AjaxHote1RoomListForDetai1.aspx?psid=&amp;MasterHotelID=701612&amp;hotel=701612&amp;EDM=F&amp;roomId=&amp;IncludeRoom=&amp;city=1&amp;showspothotel=T&amp;supplier=&amp;IsDecoupleSpotHotelAndGroup=F&amp;contrast=0&amp;brand=0&amp;startDate=2018-12-10&amp;depDate=2018-12-11&amp;IsFlash=F&amp;RequestTravelMoney=F&amp;hsids=&amp;IsJustConfirm=&amp;contyped=0&amp;priceInfo=-1&amp;equip=&amp;filter=&amp;productcode=&amp;couponList=&amp;abForHuaZhu=&amp;defaultLoad=T&amp;esfiltertag=&amp;estagid=&amp;Currency=&amp;Exchange=&amp;TmFromList=F&amp;RoomGuestCount=1,1,0&amp;eleven=1a82f6826ef6f79f8b687d200c8bc26bc83f5fd6e22ce188fa46c37e596ada0f&amp;callback=CASZypaSpRGHqDDaCsI&amp;_=1544406910108//然后进行测试缩小请求参数范围最后url：http://hotels.ctrip.com/Domestic/tool/AjaxHote1RoomListForDetai1.aspx?MasterHotelID=701615&amp;hotel=701615&amp;startDate=2018-12-10&amp;depDate=2018-12-11//范围只需要改动酒店id和入住时间即可 设置请求头参数 如果对请求的来源做了判断就需要在header设置referrer参数 寻找规律 每一行的价格都是放在tr标签中的其中一个td中，仔细观察里面有span标签有base_price属性，直接属性选择器定位获取text，后续在正则取值。 12345678@Testpublic void test2()&#123; Document document = Jsoup.connect(url2) .timeout(10000) .referrer(\"http://hotels.ctrip.com\").get(); Elements select2 = element.select(\"tr td .base_price\"); System.out.println(select2.text());&#125; 输出1234¥&lt;\\/dfn&gt;1374&lt;\\/span&gt;&lt;\\/div&gt;\\u000a\\u0009 \\u000a\\u0009\\u0009\\u000a\\u0009\\u0009\\u000a\\u0009&lt;\\/p&gt;\\u000a \\u000a &lt;\\/span&gt;\\u000a&lt;\\/p&gt;\\u000a\\u0009\\u000a&lt;\\/td&gt;\\u000a¥&lt;\\/dfn&gt;942&lt;\\/span&gt;&lt;\\/div&gt;\\u000a\\u0009 \\u000a\\u0009\\u0009\\u000a\\u0009\\u0009\\u000a\\u0009&lt;\\/p&gt;\\u000a \\u000a &lt;\\/span&gt;\\u000a&lt;\\/p&gt;\\u000a\\u0009\\u000a&lt;\\/td&gt;\\u000a¥&lt;\\/dfn&gt;1140&lt;\\/span&gt;&lt;\\/div&gt;\\u000a\\u0009 \\u000a\\u0009\\u0009\\u000a\\u0009\\u0009\\u000a\\u0009&lt;\\/p&gt;\\u000a \\u000a &lt;\\/span&gt;\\u000a&lt;\\/p&gt;\\u000a\\u0009\\u000a&lt;\\/td&gt;\\u000a¥&lt;\\/dfn&gt;849&lt;\\/span&gt;&lt;\\/div&gt;\\u000a\\u0009 \\u000a\\u0009\\u0009\\u000a\\u0009\\u0009\\u000a\\u0009&lt;\\/p&gt;\\u000a \\u000a &lt;\\/span&gt;\\u000a&lt;\\/p&gt;\\u000a\\u0009\\u000a&lt;\\/td&gt;\\u000a 正则取值 对dfn标签后的值进行匹配取值 12345678910111213141516@Testpublic void test3()&#123; Document document = Jsoup.connect(url2) .timeout(10000) .referrer(\"http://hotels.ctrip.com\").get(); Elements select = element.select(\"tr td .base_price\"); for (Element element1 : select) &#123; String text = element1.text(); //¥&lt;\\/dfn&gt;1374&lt;\\/span&gt;&lt;\\/div&gt;\\u000a\\u0009... Pattern pattern = Pattern.compile(\"dfn&gt;(\\\\d+)&lt;\"); Matcher matcher = pattern.matcher(text); if(matcher.find())&#123; System.out.println(matcher.group(1)); &#125; &#125;&#125; 输出 123413749421140849 参考 jsoup开发指南,jsoup中文使用手册,jsoup中文文档 jsoup官方文档 org.jsoup.nodes.Element的API Jsoup学习总结","categories":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/categories/爬虫/"}],"tags":[{"name":"spider","slug":"spider","permalink":"http://yoursite.com/tags/spider/"},{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"},{"name":"jsoup","slug":"jsoup","permalink":"http://yoursite.com/tags/jsoup/"}]},{"title":"Redis的学习_基础数据结构（二）(可能是最完善的整理)","slug":"redis/Redis的学习_基础数据结构（二）","date":"2018-12-05T03:57:00.000Z","updated":"2018-12-06T09:19:47.775Z","comments":true,"path":"2018/12/05/redis/Redis的学习_基础数据结构（二）/","link":"","permalink":"http://yoursite.com/2018/12/05/redis/Redis的学习_基础数据结构（二）/","excerpt":"Redis 基础数据结构Redis 有 5 种基础数据结构，分别为： string (字符串) list (列表) hash (哈希) set (集合) zset (有序集合) Redis 所有的数据结构都是以唯一的 key 字符串作为名称，然后通过这个唯一 key 值来获取相应的 value 数据。不同类型的数据结构的差异就在于 value 的结构不一样。","text":"Redis 基础数据结构Redis 有 5 种基础数据结构，分别为： string (字符串) list (列表) hash (哈希) set (集合) zset (有序集合) Redis 所有的数据结构都是以唯一的 key 字符串作为名称，然后通过这个唯一 key 值来获取相应的 value 数据。不同类型的数据结构的差异就在于 value 的结构不一样。 本文约9000字符，阅读时间约19分钟 – about 19 min read String (字符串)字符串 string 是 Redis 最简单的数据结构。 字符串结构使用非常广泛，一个常见的用途就是缓存用户信息。我们将用户信息结构体使用 JSON 序列化成字符串，然后将序列化后的字符串塞进 Redis 来缓存。同样，取用户信息会经过一次反序列化的过程。 存储容量会自动扩容 Redis 的字符串是动态字符串，是可以修改的字符串，内部结构实现上类似于 Java 的 ArrayList，采用预分配冗余空间的方式来减少内存的频繁分配，如图中所示，内部为当前字符串实际分配的空间 capacity 一般要高于实际字符串长度 len。当字符串长度小于 1M 时，扩容都是加倍现有的空间，如果超过 1M，扩容时一次只会多扩 1M 的空间。需要注意的是字符串最大长度为 512M。 操作 键值对 12345678910111213141516&gt; set name kuangerOK&gt; get name\"kuanger\"&gt; getset name kuanger2 # 取值后再赋值\"kuanger\"&gt; get name\"kuanger2\"&gt; exists name(integer) 1&gt; getrange name 0 2 # 获取字符串的[start end]的字符\"kua\"&gt; del name(integer) 1&gt; get name(nil) 批量键值对 12345678910111213&gt; set name1 kuanger1OK&gt; set name2 kuanger2OK&gt; mget name1 name2 name3 # 返回一个列表1) \"kuanger1\"2) \"kuanger2\"3) (nil)&gt; mset name1 boy name2 girl name3 unknown&gt; mget name1 name2 name31) \"boy\"2) \"girl\"3) \"unknown\" 过期和 set 命令扩展可以对 key 设置过期时间，到点自动删除，这个功能常用来控制缓存的失效时间。不过这个「自动删除」的机制是比较复杂的，后面会记录. 1234567891011121314151617181920212223242526&gt; set name kuanger&gt; get name\"kuanger\"&gt; expire name 5 # 设置5s 后过期(integer) 1 # 1表示设置成功，0表示变量ireader不存在... # wait for 5s&gt; ttl ireader # 查看寿命(integer) 4 # 还有4秒的寿命，返回-2表示变量不存在，-1表示没有设置过期时间&gt; get name(nil)&gt; setex name 5 kuanger # 5s 后过期，等价于 set+expire&gt; get name\"kuanger\"... # wait for 5s&gt; get name(nil)&gt; setnx name kuanger # 如果 name 不存在就执行 set 创建(integer) 1&gt; get name\"kuanger\"&gt; setnx name hello(integer) 0 # 因为 name 已经存在，所以 set 创建不成功&gt; get name\"kuanger\" # 没有改变 计数 如果 value 值是一个整数，还可以对它进行自增操作。自增是有范围的，它的范围是 signed long 的最大最小值，超过了这个值，Redis 会报错。 1234567891011121314&gt; set age 30OK&gt; incr age # 默认加一(integer) 31&gt; decr ireader # 等价于decrby ireader 1(integer) 30&gt; incrby age 6(integer) 36&gt; incrby age -5(integer) 31&gt; set codehole 9223372036854775807 # Long.MaxOK&gt; incr codehole(error) ERR increment or decrement would overflow List(列表)Redis 的列表相当于 Java 语言里面的 LinkedList，注意它是链表而不是数组。这意味着 list 的插入和删除操作非常快，时间复杂度为 O(1)，但是索引定位很慢，时间复杂度为 O(n)，这点让人非常意外。 当列表弹出了最后一个元素之后，该数据结构自动被删除，内存被回收。 Redis 的列表结构常用来做 异步队列 使用。将需要延后处理的任务结构体序列化成字符串塞进 Redis 的列表，另一个线程从这个列表中轮询数据进行处理。 负下标 : 链表元素的位置使用自然数0,1,2,….n-1表示，还可以使用负数-1,-2,…-n来表示，-1表示「倒数第一」，-2表示「倒数第二」，那么-n就表示第一个元素，对应的下标为0。 操作 队列／堆栈 : 链表可以从表头和表尾追加和移除元素，结合使用rpush/rpop/lpush/lpop四条指令，可以将链表作为队列或堆栈使用，左向右向进行都可以 右边进左边出：队列 123456789101112&gt; rpush books python java golang(integer) 3&gt; llen books # 使用llen指令获取链表长度(integer) 3&gt; lpop books\"python\"&gt; lpop books\"java\"&gt; lpop books\"golang\"&gt; lpop books(nil) 右边进右边出：栈 12345678910&gt; rpush books python java golang(integer) 3&gt; rpop books\"golang\"&gt; rpop books\"java\"&gt; rpop books\"python\"&gt; rpop books(nil) 修改元素 12345678&gt; rpush ireader go java python(integer) 3&gt; lset ireader 1 javascript # 使用lset指令在指定位置修改元素。OK&gt; lrange ireader 0 -11) \"go\"2) \"javascript\"3) \"python\" 插入元素linsert : 使用linsert指令在列表的中间位置插入元素, 通过方向参数before/after来显示指示前置和后置插入。 不过让人意想不到的是linsert指令并不是通过指定位置来插入，而是通过指定具体的值。这是因为在分布式环境下，列表的元素总是频繁变动的，意味着上一时刻计算的元素下标在下一时刻可能就不是你所期望的下标了。123456789&gt; rpush ireader go java python(integer) 3&gt; linsert ireader before java ruby(integer) 4&gt; lrange ireader 0 -11) \"go\"2) \"ruby\"3) \"java\"4) \"python\" 暂时还没有在实际应用中发现插入指定的应用场景 删除元素 : 列表的删除操作也不是通过指定下标来确定元素的，你需要指定删除的最大个数以及元素的值 1234567&gt; rpush ireader go java python(integer) 3&gt; lrem ireader 1 java(integer) 1&gt; lrange ireader 0 -11) \"go\"2) \"python\" 慢操作 lindex : 访问指定位置的元素, 相当于 Java 链表的 get(int index) 方法，它需要对链表进行遍历，性能随着参数index增大而变差。 lrange : 使用lrange指令来获取链表子元素列表，提供start和end下标参数 ltrim : 和字面上的含义不太一样，个人觉得它叫 lretain(保留) 更合适一些，因为 ltrim 跟的两个参数start_index和end_index定义了一个区间，在这个区间内的值，ltrim 要保留，区间之外统统砍掉。我们可以通过ltrim来实现一个定长的链表，这一点非常有用。 index : 可以为负数，index=-1表示倒数第一个元素，同样index=-2表示倒数第二个元素。 1234567891011121314151617&gt; rpush books python java golang(integer) 3&gt; lindex books 1 # O(n) 慎用\"java\"&gt; lrange books 0 -1 # 获取所有元素，O(n) 慎用1) \"python\"2) \"java\"3) \"golang\"&gt; ltrim books 1 -1 # O(n) 慎用 保留[1,-1]区间的值,第一个到最后一个OK&gt; lrange books 0 -11) \"java\"2) \"golang\"&gt; ltrim books 1 0 # 这其实是清空了整个列表，因为区间范围长度为负OK&gt; llen books(integer) 0 内部结构 快速列表 quicklist 如果再深入一点，你会发现 Redis 底层存储的还不是一个简单的 linkedlist，而是称之为快速链表 quicklist 的一个结构。 首先在列表元素较少的情况下会使用一块连续的内存存储，这个结构是 ziplist，也即是压缩列表。 它将所有的元素紧挨着一起存储，分配的是一块连续的内存。 当数据量比较多的时候才会改成 quicklist。因为普通的链表需要的附加指针空间太大，会比较浪费空间，而且会加重内存的碎片化。 比如这个列表里存的只是 int 类型的数据，结构上还需要两个额外的指针 prev 和 next 。所以 Redis 将链表和 ziplist 结合起来组成了 quicklist。也就是将多个 ziplist 使用双向指针串起来使用。这样既满足了快速的插入删除性能，又不会出现太大的空间冗余。&gt; hash (字典) Redis 的字典相当于 Java 语言里面的 HashMap，它是无序字典。内部实现结构上同 Java 的 HashMap 也是一致的，同样的数组 + 链表二维结构。第一维 hash 的数组位置碰撞时，就会将碰撞的元素使用链表串接起来。 结构上它使用二维结构，第一维是数组，第二维是链表，hash的内容key和value存放在链表中，数组里存放的是链表的头指针。 通过key查找元素时，先计算key的hashcode，然后用hashcode对数组的长度进行取模定位到链表的表头，再对链表进行遍历获取到相应的value值，链表的作用就是用来将产生了「hash碰撞」的元素串起来。 哈希的第一维数组的长度也是2^n。 不同的是，Redis 的字典的值只能是字符串，另外它们 rehash 的方式不一样，因为 Java 的 HashMap 在字典很大时，rehash 是个耗时的操作，需要一次性全部 rehash。Redis 为了高性能，不能堵塞服务，所以采用了渐进式 rehash 策略。 rehash : 重新散列, 指的是重新计算键的哈希值和索引值， 然后将键值对放置到 新计算的哈希表的指定位置上。 Redis设计与实现——哈希表的扩展与收缩 美团针对Redis Rehash机制的探索和实践 HashMap的Rehash 缩容 Redis的hash结构不但有扩容还有缩容，从这一点出发，它要比Java的HashMap要厉害一些，Java的HashMap只有扩容。缩容的原理和扩容是一致的，只不过新的数组大小要比旧数组小一倍 渐进式 rehash 会在 rehash 的同时，保留新旧两个 hash 结构，查询时会同时查询两个 hash 结构，然后在后续的定时任务中以及 hash 操作指令中，循序渐进地将旧 hash 的内容一点点迁移到新的 hash 结构中。当搬迁完成了，就会使用新的hash结构取而代之。 当 hash 移除了最后一个元素之后，该数据结构自动被删除，内存被回收。 hash 结构也可以用来存储用户信息，不同于字符串一次性需要全部序列化整个对象，hash 可以对用户结构中的每个字段单独存储。这样当我们需要获取用户信息时可以进行部分获取。而以整个字符串的形式去保存用户信息的话就只能一次性全部读取，这样就会比较浪费网络流量。 hash 也有缺点，hash 结构的存储消耗要高于单个字符串，到底该使用 hash 还是字符串，需要根据实际情况再三权衡。 操作 插入元素 123456789# hset key field value &gt; hset books java \"think in java\" # 命令行的字符串如果包含空格，要用引号括起来(integer) 1&gt; hset books golang \"concurrency in go\"(integer) 1&gt; hset books python \"python cookbook\"(integer) 1&gt; hmset books java \"effective java\" python \"learning python\" golang \"modern golang programming\" # 批量 setOK 获取元素 123456789101112131415&gt; hgetall books # entries()，key 和 value 间隔出现1) \"java\"2) \"think in java\"3) \"golang\"4) \"concurrency in go\"5) \"python\"6) \"python cookbook\"&gt; hlen books(integer) 3&gt; hget books java\"think in java\"&gt; hset books golang \"learning go programming\" # 因为是更新操作，所以返回 0(integer) 0&gt; hget books golang\"learning go programming\" 删除元素 123456&gt; hdel books java(integer) 1&gt; hdel books golang python(integer) 2&gt; del books #直接删除key 包括对应的field-value(integer) 1 同字符串对象一样，hash 结构中的单个子 key 也可以进行计数，它对应的指令是 hincrby，和 incr 使用基本一样。12345# 如果没有key-value 会直接创建默认加1&gt; hincrby user-hello age 1(integer) 1&gt; hincrby user-hello age 1(integer) 2 判断元素是否存在 : 通常我们使用hget获得key对应的value是否为空就直到对应的元素是否存在了，不过如果value的字符串长度特别大，通过这种方式来判断元素存在与否就略显浪费，这时可以使用hexists指令。1234&gt; hmset ireader go fast java fast python slowOK&gt; hexists ireader go(integer) 1 Set (集合)Redis 的集合相当于 Java 语言里面的 HashSet，它内部的键值对是无序的唯一的。它的内部实现相当于一个特殊的字典，字典中所有的 value 都是一个值NULL。 当集合中最后一个元素移除之后，数据结构自动删除，内存被回收。 set 结构可以用来存储活动中奖的用户 ID，因为有去重功能，可以保证同一个用户不会中奖两次。 操作123456789101112131415161718&gt; sadd books python(integer) 1&gt; sadd books python # 重复(integer) 0&gt; sadd books java golang(integer) 2&gt; smembers books # 注意顺序，和插入的并不一致，因为 set 是无序的1) \"java\"2) \"python\"3) \"golang\"&gt; sismember books java # 查询某个 value 是否存在，相当于 contains(o) 1表示存在，0表示不存在(integer) 1&gt; sismember books rust(integer) 0&gt; scard books # 获取长度相当于 count()(integer) 3&gt; spop books # 弹出一个\"java\" zset (有序集合)zset 可能是 Redis 提供的最为特色的数据结构，它也是在面试中面试官最爱问的数据结构。它类似于 Java 的 SortedSet 和 HashMap 的结合体，一方面它是一个 set，保证了内部 value 的唯一性，另一方面它可以给每个 value 赋予一个 score，代表这个 value 的排序权重。它的内部实现用的是一种叫做「跳跃列表」的数据结构。 zset 中最后一个 value 被移除后，数据结构自动删除，内存被回收。 zset 可以用来存粉丝列表，value 值是粉丝的用户 ID，score 是关注时间。我们可以对粉丝列表按关注时间进行排序。 zset 还可以用来存储学生的成绩，value 值是学生的 ID，score 是他的考试成绩。我们可以对成绩按分数进行排序就可以得到他的名次。 操作 命令：zadd key score value score value score value12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&gt; zadd books 9.0 \"think in java\"(integer) 1&gt; zadd books 8.9 \"java concurrency\"(integer) 1&gt; zadd books 8.6 \"java cookbook\"(integer) 1&gt; zadd books 9.1 \"think in java\" # 如果该元素已存在则会用新的分数替换原来的分数。(integer) 0 # 返回0则是更新&gt; zrange books 0 -1 # 按 score 排序列出，参数区间为排名范围 (从低到高) 不带score1) \"java cookbook\"2) \"java concurrency\"3) \"think in java\"&gt; zrange books 0 -1 withscores # 按 score 排序列出，参数区间为排名范围 (从低到高) 带score1) \"java cookbook\"2) \"8.6\"3) \"java concurrency\"4) \"8.9\"5) \"think in java\"6) \"9.0\"&gt; zrevrange books 0 -1 # 按 score 逆序列出，参数区间为排名范围 (从高到低) 不带score1) \"think in java\"2) \"java concurrency\"3) \"java cookbook\"&gt; zrevrange books 0 -1 withscores # 按 score 逆序列出，参数区间为排名范围 (从高到低) 带score1) \"think in java\"2) \"9.0\"3) \"java concurrency\"4) \"8.9\"5) \"java cookbook\"6) \"9.0\"&gt; zcard books # 相当于 count()(integer) 3&gt; zscore books \"java concurrency\" # 获取指定 value 的 score\"8.9000000000000004\" # 内部 score 使用 double 类型进行存储，所以存在小数点精度问题&gt; zrank books \"java concurrency\" # 排名(integer) 1&gt; zrangebyscore books 0 8.91 # 根据分值区间遍历 zset1) \"java cookbook\"2) \"java concurrency\"&gt; zrangebyscore books -inf 8.91 withscores # 根据分值区间 (-∞, 8.91] 遍历 zset，同时返回分值。inf 代表 infinite，无穷大的意思。1) \"java cookbook\"2) \"8.5999999999999996\"3) \"java concurrency\"4) \"8.9000000000000004\"&gt; zrem books \"java concurrency\" # 删除 value(integer) 1&gt; zrange books 0 -11) \"java cookbook\"2) \"think in java\" 跳跃列表zset 内部的排序功能是通过「跳跃列表」数据结构来实现的，它的结构非常特殊，也比较复杂。 因为 zset 要支持随机的插入和删除，所以它不好使用数组来表示。我们先看一个普通的链表结构。 我们需要这个链表按照 score 值进行排序。这意味着当有新元素需要插入时，要定位到特定位置的插入点，这样才可以继续保证链表是有序的。通常我们会通过二分查找来找到插入点，但是二分查找的对象必须是数组，只有数组才可以支持快速位置定位，链表做不到，那该怎么办？ 想想一个创业公司，刚开始只有几个人，团队成员之间人人平等，都是联合创始人。随着公司的成长，人数渐渐变多，团队沟通成本随之增加。这时候就会引入组长制，对团队进行划分。每个团队会有一个组长。开会的时候分团队进行，多个组长之间还会有自己的会议安排。公司规模进一步扩展，需要再增加一个层级 —— 部门，每个部门会从组长列表中推选出一个代表来作为部长。部长们之间还会有自己的高层会议安排。 跳跃列表就是类似于这种层级制，最下面一层所有的元素都会串起来。然后每隔几个元素挑选出一个代表来，再将这几个代表使用另外一级指针串起来。然后在这些代表里再挑出二级代表，再串起来。最终就形成了金字塔结构。 想想你老家在世界地图中的位置：亚洲–&gt;中国-&gt;XX省-&gt;XX市-&gt;XX县-&gt;XX镇-&gt;XX村-&gt;xxxx号，也是这样一个类似的结构。 「跳跃列表」之所以「跳跃」，是因为内部的元素可能「身兼数职」，比如上图中间的这个元素，同时处于 L0、L1 和 L2 层，可以快速在不同层次之间进行「跳跃」。 定位插入点时，先在顶层进行定位，然后下潜到下一级定位，一直下潜到最底层找到合适的位置，将新元素插进去。你也许会问，那新插入的元素如何才有机会「身兼数职」呢？ 跳跃列表采取一个随机策略来决定新元素可以兼职到第几层。 首先 L0 层肯定是 100% 了，L1 层只有 50% 的概率，L2 层只有 25% 的概率，L3 层只有 12.5% 的概率，一直随机到最顶层 L31 层。绝大多数元素都过不了几层，只有极少数元素可以深入到顶层。列表中的元素越多，能够深入的层次就越深，能进入到顶层的概率就会越大。 通俗的理解就是 : 需要从 header 的最高层开始遍历找到第一个节点 (最后一个比「我」小的元素)，然后从这个节点开始降一层再遍历找到第二个节点 (最后一个比「我」小的元素)，然后一直降到最底层进行遍历就找到了期望的节点 ( 最底层的最后一个比我「小」的元素 )。 我们将中间经过的一系列节点称之为 「搜索路径」 ，它是从最高层一直到最底层的每一层最后一个比「我」小的元素节点列表。 有了这个搜索路径，我们就可以插入这个新节点了。不过这个插入过程也不是特别简单。因为新插入的节点到底有多少层，得有个算法来分配一下，跳跃列表使用的是随机算法 参考 老钱的《Redis 深度历险：核心原理与应用实践》 通俗易懂的Redis数据结构基础教程","categories":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}]},{"title":"Redis的学习_开篇（一）","slug":"redis/Redis的学习_介绍（一）","date":"2018-12-04T03:56:55.000Z","updated":"2018-12-05T09:34:06.747Z","comments":true,"path":"2018/12/04/redis/Redis的学习_介绍（一）/","link":"","permalink":"http://yoursite.com/2018/12/04/redis/Redis的学习_介绍（一）/","excerpt":"摘要最近对 老钱 的 《Redis 深度历险：核心原理与应用实践》进行学习，并且一同做笔记，所以后续的Redis博客都是基于《Redis 深度历险：核心原理与应用实践》和其他的文著，再结合自己的所学到的进行结合，然后进行笔记记录，所以在此统一声明。","text":"摘要最近对 老钱 的 《Redis 深度历险：核心原理与应用实践》进行学习，并且一同做笔记，所以后续的Redis博客都是基于《Redis 深度历险：核心原理与应用实践》和其他的文著，再结合自己的所学到的进行结合，然后进行笔记记录，所以在此统一声明。 声明：笔记的参考源自钱文品老师所著《Redis 深度历险：核心原理和应用实践》，只供个人学习所用。 简单介绍安装安装Redis可以选择这5种方式： 使用 Docker 安装。 通过 Github 源码编译。 直接安装 apt-get install(Ubuntu)、yum install(RedHat) 或者 brew install(Mac)。 如果读者懒于安装操作，也可以使用网页版的 Web Redis 直接体验。 可以下载Redis绿色版直接解压可使用 操作 Docker 方式 123456# 拉取 redis 镜像&gt; docker pull redis# 运行 redis 容器&gt; docker run --name myredis -d -p6379:6379 redis# 执行容器中的 redis-cli，可以直接使用命令行操作 redis&gt; docker exec -it myredis redis-cli Github 源码编译方式 12345678910# 下载源码&gt; git clone --branch 2.8 --depth 1 git@github.com:antirez/redis.git&gt; cd redis# 编译&gt; make&gt; cd src# 运行服务器，daemonize表示在后台运行&gt; ./redis-server --daemonize yes# 运行命令行&gt; ./redis-cli 直接安装方式 12345678# mac&gt; brew install redis# ubuntu&gt; apt-get install redis# redhat&gt; yum install redis# 运行客户端&gt; redis-cli 扩展阅读 一篇Redis相关的面试文章","categories":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"http://yoursite.com/tags/Redis/"}]},{"title":"之前的笔记","slug":"something/之前的笔记","date":"2018-12-04T02:54:55.000Z","updated":"2018-12-05T02:13:00.782Z","comments":true,"path":"2018/12/04/something/之前的笔记/","link":"","permalink":"http://yoursite.com/2018/12/04/something/之前的笔记/","excerpt":"","text":"每天我都会阅读一到两篇的博客、人家的经验总结等等，看完后自己会做一下小笔记，笔记都做在有道云，因为手机电脑都可以随时看到，挺方便的，然后过往分享了一下笔记在 CSDN 上面， 我就不搬过来了，有兴趣可以点击去阅读：https://blog.csdn.net/Kato_op","categories":[{"name":"Talk","slug":"Talk","permalink":"http://yoursite.com/categories/Talk/"}],"tags":[{"name":"笔记","slug":"笔记","permalink":"http://yoursite.com/tags/笔记/"}]},{"title":"CAP理论","slug":"knowledge/CAP理论","date":"2018-12-03T03:54:55.000Z","updated":"2018-12-05T08:54:36.049Z","comments":true,"path":"2018/12/03/knowledge/CAP理论/","link":"","permalink":"http://yoursite.com/2018/12/03/knowledge/CAP理论/","excerpt":"本文约1500字符，阅读时间约3分钟 – about 3 min read 前言对于分布式概念就不详细介绍了，分布式简单理解就是：一个业务分拆多个子业务，部署在不同的服务器上。 一般来说，一个子业务我们称为节点。 Content 首先，我们来看一下CAP分别代表的是什么意思： C：数据一致性(consistency) 所有节点拥有数据的最新版本 A：可用性(availability) 数据具备高可用性 P：分区容错性(partition-tolerance) 容忍网络出现分区，分区之间网络不可达。","text":"本文约1500字符，阅读时间约3分钟 – about 3 min read 前言对于分布式概念就不详细介绍了，分布式简单理解就是：一个业务分拆多个子业务，部署在不同的服务器上。 一般来说，一个子业务我们称为节点。 Content 首先，我们来看一下CAP分别代表的是什么意思： C：数据一致性(consistency) 所有节点拥有数据的最新版本 A：可用性(availability) 数据具备高可用性 P：分区容错性(partition-tolerance) 容忍网络出现分区，分区之间网络不可达。 下面有三个节点(它们是集群的)，此时三个节点都能够相互通信： 如果出现故障由于我们的系统是分布式的，节点之间的通信是通过网络来进行的。只要是分布式系统，那很有可能会出现一种情况：因为一些故障，使得有些节点之间不连通了，整个网络就分成了几块区域。 数据就散布在了这些不连通的区域中，这就叫分区 现在出现了网络分区后，此时有一个请求过来了，想要注册一个账户。 此时我们节点一和节点三是不可通信的，这就有了抉择： 如果允许当前用户注册一个账户，此时注册的记录数据只会在节点一和节点二或者节点二和节点三同步，因为节点一和节点三的记录不能同步的。 这种情况其实就是选择了可用性(availability)，抛弃了数据一致性(consistency) 如果不允许当前用户注册一个账户(就是要等到节点一和节点三恢复通信)。节点一和节点三一旦恢复通信，我们就可以保证节点拥有的数据是最新版本。 这种情况其实就是抛弃了可用性(availability)，选择了数据一致性(consistency) 再次梳理一下CAP理论一般我们说的分布式系统，P：分区容错性(partition-tolerance)这个是必需的，这是客观存在的。 CAP是无法完全兼顾的，从上面的例子也可以看出，我们可以选AP，也可以选CP。但是，要注意的是：不是说选了AP，C就完全抛弃了。不是说选了CP，A就完全抛弃了！ 在CAP理论中，C所表示的一致性是强一致性(每个节点的数据都是最新版本)，其实一致性还有其他级别的： 弱一致性：弱一致性是相对于强一致性而言，它不保证总能得到最新的值； 最终一致性(eventual consistency)：放宽对时间的要求，在被调完成操作响应后的某个时间点，被调多个节点的数据最终达成一致 可用性的值域可以定义成0到100%的连续区间。 所以，CAP理论定义的其实是在容忍网络分区的条件下，“强一致性”和“极致可用性”无法同时达到。 参考资料： 摘取文章片段 ： https://mp.weixin.qq.com/s/oTJaEFrHQi_7R68n26yfew CAP理论中的P到底是个什么意思？https://www.zhihu.com/question/54105974 浅谈分布式系统的基本问题：可用性与一致性：https://m.aliyun.com/yunqi/articles/2709 分布式系统的CAP理论：http://www.hollischuang.com/archives/666 为什么CAP理论在舍弃P的情况下，可以有完美的CA？https://www.zhihu.com/question/285878189 不懂点CAP理论，你好意思说你是做分布式的吗？http://www.yunweipai.com/archives/8432.html 扩展阅读： 浅谈分布式事务：https://m.aliyun.com/yunqi/articles/230242","categories":[{"name":"分布式/微服务理论","slug":"分布式-微服务理论","permalink":"http://yoursite.com/categories/分布式-微服务理论/"}],"tags":[{"name":"CAP理论","slug":"CAP理论","permalink":"http://yoursite.com/tags/CAP理论/"}]},{"title":"一个学习Git的有趣网站","slug":"something/一个学习Git的有趣网站","date":"2018-11-30T06:54:55.000Z","updated":"2018-12-05T02:13:00.780Z","comments":true,"path":"2018/11/30/something/一个学习Git的有趣网站/","link":"","permalink":"http://yoursite.com/2018/11/30/something/一个学习Git的有趣网站/","excerpt":"一个非常有趣的小网站，把Git的命令和知识点做成类似于游戏闯关一样，生动有效地帮助初步学习Git的朋友们。","text":"一个非常有趣的小网站，把Git的命令和知识点做成类似于游戏闯关一样，生动有效地帮助初步学习Git的朋友们。 网站链接：url: https://learngitbranching.js.org/?demo 在里面你能执行相应的命令，还能看到每个命令的执行情况； 通过一系列刺激的关卡挑战，逐步深入的学习 Git 的强大功能，在这个过程中你可能还会发现一些有意思的事情。 尝试了一番 他把Git的知识点都拆分成每一小块关卡，每个关卡他都会有教程和知识点的数列，如果你是初学者，从第一关开始逐个向后挑战就是了。 而如果你已经入门了，可以略过前面，直接挑战后面更有难度的关卡。 选择一个关卡之后，左边有弹框给你输入git的命令，根据知识点输入命令就可以完成了，然后可以生动有效地帮助你学习，Nice~","categories":[{"name":"Version Control","slug":"Version-Control","permalink":"http://yoursite.com/categories/Version-Control/"}],"tags":[{"name":"Git","slug":"Git","permalink":"http://yoursite.com/tags/Git/"},{"name":"funny","slug":"funny","permalink":"http://yoursite.com/tags/funny/"},{"name":"Version Control","slug":"Version-Control","permalink":"http://yoursite.com/tags/Version-Control/"}]},{"title":"Java问题快速排查","slug":"question/Java问题快速排查","date":"2018-11-30T02:54:55.000Z","updated":"2018-12-05T14:12:48.979Z","comments":true,"path":"2018/11/30/question/Java问题快速排查/","link":"","permalink":"http://yoursite.com/2018/11/30/question/Java问题快速排查/","excerpt":"这是一篇来源于阿里内部技术论坛的文章，原文在阿里内部获得一致好评。","text":"这是一篇来源于阿里内部技术论坛的文章，原文在阿里内部获得一致好评。 作者：红魔七号 链接：https://yq.aliyun.com/articles/69520?utm_content=m_10360 参考文章： https://mp.weixin.qq.com/s/TMAe3AyOfUQIOf0hGurR4A 原谅小弟不才，有问题可以下方留言指教~ 本文约6000字符，阅读时间约15分钟 – about 15 min read 前言平时的工作中经常碰到很多疑难问题的处理，在解决问题的同时，有一些工具起到了相当大的作用，在此书写下来，一是作为笔记，可以让自己后续忘记了可快速翻阅，二是分享，希望看到此文的同学们可以拿出自己日常觉得帮助很大的工具，大家一起进步。 Linux命令类tail最常用的tail -f 1tail -300f shopbase.log #倒数300行并进入实时监听文件写入模式 grep12345678910grep forest f.txt #文件查找grep forest f.txt cpf.txt #多文件查找grep 'log' /home/admin -r -n #目录下查找所有符合关键字的文件cat f.txt | grep -i shopbase grep 'shopbase' /home/admin -r -n --include *.&#123;vm,java&#125; #指定文件后缀grep 'shopbase' /home/admin -r -n --exclude *.&#123;vm,java&#125; #反匹配seq 10 | grep 5 -A 3 #上匹配seq 10 | grep 5 -B 3 #下匹配seq 10 | grep 5 -C 3 #上下匹配，平时用这个就妥了cat f.txt | grep -c 'SHOPBASE' awk 基本命令 123456awk '&#123;print $4,$6&#125;' f.txtawk '&#123;print NR,$0&#125;' f.txt cpf.txt awk '&#123;print FNR,$0&#125;' f.txt cpf.txtawk '&#123;print FNR,FILENAME,$0&#125;' f.txt cpf.txtawk '&#123;print FILENAME,\"NR=\"NR,\"FNR=\"FNR,\"$\"NF\"=\"$NF&#125;' f.txt cpf.txtecho 1:2:3:4 | awk -F: '&#123;print $1,$2,$3,$4&#125;' 匹配 1234awk '/ldb/ &#123;print&#125;' f.txt #匹配ldbawk '!/ldb/ &#123;print&#125;' f.txt #不匹配ldbawk '/ldb/ &amp;&amp; /LISTEN/ &#123;print&#125;' f.txt #匹配ldb和LISTENawk '$5 ~ /ldb/ &#123;print&#125;' f.txt #第五列匹配ldb 内建变量 NR:NR表示从awk开始执行后，按照记录分隔符读取的数据次数，默认的记录分隔符为换行符，因此默认的就是读取的数据行数，NR可以理解为Number of Record的缩写。 FNR:在awk处理多个输入文件的时候，在处理完第一个文件后，NR并不会从1开始，而是继续累加，因此就出现了FNR，每当处理一个新文件的时候，FNR就从1开始计数，FNR可以理解为File Number of Record。 NF: NF表示目前的记录被分割的字段的数目，NF可以理解为Number of Field。 find12345678910111213sudo -u admin find /home/admin /tmp /usr -name \\*.log #(多个目录去找)find . -iname \\*.txt #(大小写都匹配)find . -type d #(当前目录下的所有子目录)find /usr -type l #(当前目录下所有的符号链接)find /usr -type l -name \"z*\" -ls #(符号链接的详细信息 eg:inode,目录)find /home/admin -size +250000k #(超过250000k的文件，当然+改成-就是小于了)find /home/admin f -perm 777 -exec ls -l &#123;&#125; \\; #(按照权限查询文件)find /home/admin -atime -1 #1天内访问过的文件find /home/admin -ctime -1 #1天内状态改变过的文件 find /home/admin -mtime -1 #1天内修改过的文件find /home/admin -amin -1 #1分钟内访问过的文件find /home/admin -cmin -1 #1分钟内状态改变过的文件 find /home/admin -mmin -1 #1分钟内修改过的文件 pgm批量查询vm-shopbase满足条件的日志1pgm -A -f vm-shopbase 'cat /home/admin/shopbase/logs/shopbase.log.2017-01-17|grep 2069861630' tsar tsar是alibabak开发的一个采集工具，可以上github去clone安装。他可以将历史收集到的数据持久化在磁盘上，所以我们快速来查询历史的系统数据 https://github.com/alibaba/tsar 12345678910tsar ###可以查看最近一天的各项指标tsar --live ###可以查看实时指标，默认五秒一刷tsar -d 20161218 ###指定查看某天的数据，貌似最多只能看四个月的数据tsar --memtsar --loadtsar --cpu###当然这个也可以和-d参数配合来查询某天的单个指标的情况 toptop除了看一些基本信息之外，剩下的就是配合来查询vm的各种问题了12ps -ef | grep javatop -H -p pid 获得线程10进制转16进制后jstack去抓看这个线程到底在干啥 其他12netstat -nat|awk '&#123;print $6&#125;'|sort|uniq -c|sort -rn #查看当前连接，注意close_wait偏高的情况，比如如下 排查利器btrace首当其冲的要说的是btrace。真是生产环境&amp;预发的排查问题大杀器。更详情可以移步到github：https://github.com/btraceio/btrace 查看当前谁调用了ArrayList的add方法，同时只打印当前ArrayList的size大于500的线程调用栈 监控当前服务方法被调用时返回的值以及请求的参数 注意: 经过观察，1.3.9的release输出不稳定，要多触发几次才能看到正确的结果 正则表达式匹配trace类时范围一定要控制，否则极有可能出现跑满CPU导致应用卡死的情况 由于是字节码注入的原理，想要应用恢复到正常情况，需要重启应用。 Greys说几个挺棒的功能(部分功能和btrace重合): sc -df xxx: 输出当前类的详情,包括源码位置和classloader结构 trace class method: 相当喜欢这个功能! 很早前可以早JProfiler看到这个功能。打印出当前方法调用的耗时情况，细分到每个方法。 javOSize就说一个功能 classes：通过修改了字节码，改变了类的内容，即时生效。 所以可以做到快速的在某个地方打个日志看看输出，缺点是对代码的侵入性太大。但是如果自己知道自己在干嘛，的确是不错的玩意儿。 其他功能Greys和btrace都能很轻易做的到，不说了。 JProfiler之前判断许多问题要通过 JProfiler，但是现在 Greys和 btrace基本都能搞定了。再加上出问题的基本上都是生产环境(网络隔离)，所以基本不怎么使用了，但是还是要标记一下。官网请移步https://www.ej-technologies.com/products/jprofiler/overview.html Java Tooljps m:输出主函数传入的参数. 下的hello 就是在执行程序时从命令行输入的参数 l: 输出应用程序主类完整package名称或jar完整名称. v: 列出jvm参数, -Xms20m -Xmx50m是启动程序指定的jvm参数 V: 输出通过.hotsportrc或-XX:Flags=指定的jvm参数1sudo -u admin /opt/taobao/java/bin/jps -mlvV jstack 普通用法:1sudo -u admin /opt/taobao/install/ajdk-8_1_1_fp1-b52/bin/jstack 2815 native+java栈:1sudo -u admin /opt/taobao/install/ajdk-8_1_1_fp1-b52/bin/jstack -m 2815 jinfo可看系统启动的参数，如下1sudo -u admin /opt/taobao/install/ajdk-8_1_1_fp1-b52/bin/jinfo -flags 2815 jmap 查看堆的情况 1sudo -u admin /opt/taobao/install/ajdk-8_1_1_fp1-b52/bin/jmap -heap 2815 dump 1sudo -u admin /opt/taobao/install/ajdk-8_1_1_fp1-b52/bin/jmap -dump:live,format=b,file=/tmp/heap2.bin 2815 or1sudo -u admin /opt/taobao/install/ajdk-8_1_1_fp1-b52/bin/jmap -dump:format=b,file=/tmp/heap3.bin 2815 看看堆都被谁占了? 再配合zprofiler和btrace，排查问题简直是如虎添翼 1sudo -u admin /opt/taobao/install/ajdk-8_1_1_fp1-b52/bin/jmap -histo 2815 | head -10 jstatjstat参数众多，但是使用一个就够了1sudo -u admin /opt/taobao/install/ajdk-8_1_1_fp1-b52/bin/jstat -gcutil 2815 1000 jdb时至今日，jdb也是经常使用的。jdb可以用来预发debug,假设你预发的java_home是/opt/taobao/java/，远程调试端口是8000.那么sudo -u admin /opt/taobao/java/bin/jdb -attach 8000. 出现以上代表jdb启动成功。后续可以进行设置断点进行调试。具体参数可见oracle官方说明http://docs.oracle.com/javase/7/docs/technotes/tools/windows/jdb.html CHLSDBCHLSDB感觉很多情况下可以看到更好玩的东西，不详细叙述了。 查询资料听说jstack和jmap等工具就是基于它的。1sudo -u admin /opt/taobao/java/bin/java -classpath /opt/taobao/java/lib/sa-jdi.jar sun.jvm.hotspot.CLHSDB 更详细的可见R大此贴http://rednaxelafx.iteye.com/blog/1847971 jar包冲突把这个单独写个大标题不过分吧？每个人或多或少都处理过这种烦人的case。我特么下边这么多方案不信就搞不定你?1mvn dependency:tree &gt; ~/dependency.txt 打出所有依赖 1mvn dependency:tree -Dverbose -Dincludes=groupId:artifactId 只打出指定groupId和artifactId的依赖关系 1-XX:+TraceClassLoading vm启动脚本加入。在tomcat启动脚本中可见加载类的详细信息 1-verbose vm启动脚本加入。在tomcat启动脚本中可见加载类的详细信息 1greys:sc greys的sc命令也能清晰的看到当前类是从哪里加载过来的 1tomcat-classloader-locate 通过以下url可以获知当前类是从哪里加载的 1curl http://localhost:8006/classloader/locate?class=org.apache.xerces.xs.XSObjec 其他dmesg如果发现自己的java进程悄无声息的消失了，几乎没有留下任何线索，那么dmesg一发，很有可能有你想要的。1sudo dmesg|grep -i kill|less 去找关键字oom_killer。找到的结果类似如下:12345[6710782.021013] java invoked oom-killer: gfp_mask=0xd0, order=0, oom_adj=0, oom_scoe_adj=0[6710782.070639] [&lt;ffffffff81118898&gt;] ? oom_kill_process+0x68/0x140 [6710782.257588] Task in /LXC011175068174 killed as a result of limit of /LXC011175068174 [6710784.698347] Memory cgroup out of memory: Kill process 215701 (java) score 854 or sacrifice child [6710784.707978] Killed process 215701, UID 679, (java) total-vm:11017300kB, anon-rss:7152432kB, file-rss:1232kB 以上表明，对应的java进程被系统的OOM Killer给干掉了，得分为854.解释一下OOM killer（Out-Of-Memory killer），该机制会监控机器的内存资源消耗。当机器内存耗尽前，该机制会扫描所有的进程（按照一定规则计算，内存占用，时间等），挑选出得分最高的进程，然后杀死，从而保护机器。 dmesg日志时间转换公式: log实际时间=格林威治1970-01-01+(当前时间秒数-系统启动至今的秒数+dmesg打印的log时间)秒数：1date -d \"1970-01-01 UTC `echo \"$(date +%s)-$(cat /proc/uptime|cut -f 1 -d' ')+12288812.926194\"|bc ` seconds\" 剩下的，就是看看为什么内存这么大，触发了OOM-Killer了。 eclipseMAT可作为eclipse的插件，也可作为单独的程序打开。详情请移步http://www.eclipse.org/mat/ Plugin of intellij idea key promoter maven helper 分析maven依赖的好帮手。 RateLimiter想要精细的控制QPS? 比如这样一个场景，你调用某个接口，对方明确需要你限制你的QPS在400之内你怎么控制？这个时候RateLimiter就有了用武之地。 详情可移步http://ifeve.com/guava-ratelimite Thanks~","categories":[{"name":"问题解决","slug":"问题解决","permalink":"http://yoursite.com/categories/问题解决/"}],"tags":[{"name":"Java","slug":"Java","permalink":"http://yoursite.com/tags/Java/"},{"name":"问题解决","slug":"问题解决","permalink":"http://yoursite.com/tags/问题解决/"}]},{"title":"Hello World","slug":"something/Hello-World","date":"2018-11-28T07:47:17.000Z","updated":"2018-12-05T02:13:00.782Z","comments":true,"path":"2018/11/28/something/Hello-World/","link":"","permalink":"http://yoursite.com/2018/11/28/something/Hello-World/","excerpt":"Preface Record the birth of my website 记录一下我的网站的诞生~","text":"Preface Record the birth of my website 记录一下我的网站的诞生~ Homework is stupid,the whole point is to get less of it——Rick 注意： 阅读文章的时候可以点击左上角的Logo，可以对文章的字体调整和选择夜间模式。 PrintHello World! 1System.out.print(\"Hello world\");","categories":[{"name":"welcome","slug":"welcome","permalink":"http://yoursite.com/categories/welcome/"}],"tags":[{"name":"welcome","slug":"welcome","permalink":"http://yoursite.com/tags/welcome/"}]}]}